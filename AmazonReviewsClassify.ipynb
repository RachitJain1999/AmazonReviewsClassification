{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Score                                               Text\n",
      "0   1      1  I have bought several of the Vitality canned d...\n",
      "1   2      0  Product arrived labeled as Jumbo Salted Peanut...\n",
      "2   3      1  This is a confection that has been around a fe...\n",
      "3   4      0  If you are looking for the secret ingredient i...\n",
      "4   5      1  Great taffy at a great price.  There was a wid...\n",
      "5   6      1  I got a wild hair for taffy and ordered this f...\n",
      "6   7      1  This saltwater taffy had great flavors and was...\n",
      "7   8      1  This taffy is so good.  It is very soft and ch...\n",
      "8   9      1  Right now I'm mostly just sprouting this so my...\n",
      "9  10      1  This is a very healthy dog food. Good for thei...\n"
     ]
    }
   ],
   "source": [
    "#Basic importing\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "con = sqlite3.connect('Amazon_reviews.sqlite')\n",
    "data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score!=3 LIMIT 5000\"\"\", con)\n",
    "\n",
    "actual_score=data[\"Score\"]\n",
    "positiveNegative=actual_score.map(partition)\n",
    "data[\"Score\"]=positiveNegative\n",
    "data=data.drop([\"UserId\",\"ProductId\",\"HelpfulnessNumerator\",\"HelpfulnessDenominator\",\"ProfileName\",\"Summary\",\"Time\"],axis=1)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Score                                               Text\n",
      "0   1      1  [bought, sever, vital, can, dog, food, product...\n",
      "1   2      0  [product, arriv, label, jumbo, salt, peanut, p...\n",
      "2   3      1  [confect, around, centuri, light, pillowi, cit...\n",
      "3   4      0  [look, secret, ingredi, robitussin, believ, fo...\n",
      "4   5      1  [great, taffi, great, price, wide, assort, yum...\n",
      "5   6      1  [got, wild, hair, taffi, order, five, pound, b...\n",
      "6   7      1  [saltwat, taffi, great, flavor, soft, chewi, c...\n",
      "7   8      1  [taffi, good, soft, chewi, flavor, amaz, would...\n",
      "8   9      1  [right, im, most, sprout, cat, eat, grass, lov...\n",
      "9  10      1  [healthi, dog, food, good, digest, also, good,...\n"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop = set(stopwords.words('english')) \n",
    "sno = nltk.stem.SnowballStemmer('english') \n",
    "\n",
    "def cleanhtml(sentence): \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "\n",
    "textArray=np.array(data[\"Text\"])\n",
    "cleanedTextArray=[]\n",
    "for sent in textArray:\n",
    "    sent=cleanhtml(sent);\n",
    "    sent=cleanpunc(sent);\n",
    "    sentArray=sent.split()\n",
    "    cleanedSent=[]\n",
    "    for word in sentArray:\n",
    "        word=word.lower();\n",
    "        if(word not in stop):\n",
    "            word=sno.stem(word)\n",
    "            cleanedSent.append(word);\n",
    "    cleanedTextArray.append(cleanedSent)\n",
    "\n",
    "data[\"Text\"] = cleanedTextArray\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive negative data pts:  4187 813\n",
      "Number of positive negative data pts after Upsampling:  4187 4878\n",
      "   Score                                               Text\n",
      "0      1  [bought, sever, vital, can, dog, food, product...\n",
      "1      1  [confect, around, centuri, light, pillowi, cit...\n",
      "2      1  [great, taffi, great, price, wide, assort, yum...\n",
      "3      1  [got, wild, hair, taffi, order, five, pound, b...\n",
      "4      1  [saltwat, taffi, great, flavor, soft, chewi, c...\n",
      "5      1  [taffi, good, soft, chewi, flavor, amaz, would...\n",
      "6      1  [right, im, most, sprout, cat, eat, grass, lov...\n",
      "7      1  [healthi, dog, food, good, digest, also, good,...\n",
      "8      1  [dont, know, cactus, tequila, uniqu, combin, i...\n",
      "9      1  [one, boy, need, lose, weight, didnt, put, foo...\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "#upsampling\n",
    "data1 = data[data[\"Score\"]==1]\n",
    "data0 = data[data[\"Score\"]==0]\n",
    "print(\"Number of positive negative data pts: \",data1.shape[0], data0.shape[0])\n",
    "data0new = pd.concat([data0,data0,data0,data0,data0,data0],ignore_index=True)\n",
    "\n",
    "data = pd.concat([data1,data0new],ignore_index=True)\n",
    "data = data.drop(\"Id\",axis=1)\n",
    "print(\"Number of positive negative data pts after Upsampling: \",data1.shape[0],data[data[\"Score\"]==0].shape[0])\n",
    "print(data.head(10))\n",
    "\n",
    "#Train-Test Split\n",
    "data_L= data[\"Score\"]\n",
    "data  = data.drop(\"Score\",axis=1)\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(data,data_L,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8158/8158 [00:37<00:00, 219.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.431903 -0.381995  0.378593  0.523820  0.284583 -0.207679 -0.177397   \n",
      "1  0.575280 -0.680137  0.293370  0.591984  0.454178 -0.477862 -0.291293   \n",
      "2  0.308826 -0.832527  0.698802  0.245989  0.033335 -0.172367 -0.668512   \n",
      "3  0.339948 -0.658952  0.527192  0.199352  0.311589 -0.315483 -0.282666   \n",
      "4  0.355836 -0.568258  0.296901  0.583916  0.445909 -0.350972  0.012591   \n",
      "5  0.442231 -0.625233  0.235512  0.640576  0.417766 -0.269260 -0.169581   \n",
      "6  0.737685 -0.599947  0.232405  0.819594  0.498369 -0.531497 -0.190363   \n",
      "7  0.541054 -0.542808  0.399283  0.435575  0.424205 -0.466830 -0.274356   \n",
      "8  0.497757 -0.740315  0.218907  0.108493  0.249225 -0.335615 -0.567747   \n",
      "9  0.753847 -0.580651  0.176100  1.233916  0.662455 -0.949763 -0.057467   \n",
      "\n",
      "         7         8         9   ...        40        41        42        43  \\\n",
      "0 -0.072036  0.247916 -0.334947  ...  0.324443  0.049323 -0.474039 -0.570390   \n",
      "1  0.219156  0.452372 -0.044987  ...  0.310391  0.279467 -0.338601 -0.673707   \n",
      "2 -0.014133 -0.028430 -0.744390  ...  0.570878 -0.120540 -0.606413 -0.394302   \n",
      "3  0.269144  0.540186  0.107392  ...  0.309722  0.097520 -0.335803 -0.568978   \n",
      "4  0.563579  0.227666  0.039674  ...  0.129741 -0.069201 -0.406532 -0.674922   \n",
      "5  0.062921  0.389683 -0.302671  ...  0.331503  0.089354 -0.432683 -0.452889   \n",
      "6  0.464657  0.293025 -0.404350  ...  0.397739 -0.123289 -0.362761 -0.499840   \n",
      "7  0.494279  0.397697  0.113442  ...  0.322014  0.012101 -0.591821 -0.615984   \n",
      "8  0.221403  0.575402 -0.136849  ...  0.410918  0.137188 -0.231408 -0.390920   \n",
      "9  0.547125  0.316873 -0.269553  ...  0.343672 -0.344381 -0.419542 -0.545266   \n",
      "\n",
      "         44        45        46        47        48        49  \n",
      "0  0.782804 -0.167744  0.732960 -0.072769  0.325079  0.042855  \n",
      "1  0.601184  0.096329  0.399821 -0.118262  0.130891 -0.063732  \n",
      "2  1.231795 -0.421412  0.313968 -0.474119  0.784085  0.342736  \n",
      "3  0.613079 -0.132773  0.080993 -0.283358  0.330589  0.103293  \n",
      "4  0.385887  0.144703  0.515525 -0.129174  0.162945 -0.080221  \n",
      "5  0.666711 -0.031503  0.562230  0.096811  0.145956  0.042317  \n",
      "6  0.585072  0.330513  1.246753 -0.044051 -0.070523 -0.150852  \n",
      "7  0.760611  0.077835  0.547560 -0.464692  0.164492 -0.018081  \n",
      "8  0.782819  0.080032  0.265791 -0.140908  0.123143 -0.098339  \n",
      "9  0.274398  0.727093  1.465490  0.177515 -0.341975  0.117656  \n",
      "\n",
      "[10 rows x 50 columns]\n",
      "(8158, 50)\n"
     ]
    }
   ],
   "source": [
    "#Vectorize Text\n",
    "\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "\n",
    "list_of_sent = X_train[\"Text\"]\n",
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, workers=4)\n",
    "w2v_words=list(w2v_model.wv.vocab)\n",
    "\n",
    "listof_sent_vec=[]\n",
    "#tqdm is for improving speed\n",
    "#Vectorization and normalization both going on\n",
    "for sent in tqdm(list_of_sent): \n",
    "    sent_vec = np.zeros(50) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    listof_sent_vec.append(sent_vec)\n",
    "    \n",
    "list_col=tuple(range(50))\n",
    "X_train=pd.DataFrame(data=listof_sent_vec, columns=list_col)\n",
    "print(X_train.head(10))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 907/907 [00:03<00:00, 232.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.031603 -0.847403  0.133999  0.892748  0.365950 -0.633642  0.194094   \n",
      "1  0.270566 -0.520761  0.296037  0.781815  0.276381 -0.426343 -0.254261   \n",
      "2  0.331553 -0.546859  0.380111  0.309448  0.256770 -0.313957 -0.288418   \n",
      "3  0.479919 -0.562102  0.532904  0.315516  0.244812 -0.287422 -0.343223   \n",
      "4  0.342021 -0.731478  0.610917  0.290220  0.231881 -0.127621 -0.360007   \n",
      "5  0.239157 -0.626500  0.319459  0.144555  0.265537 -0.271952 -0.294281   \n",
      "6  1.119164 -0.463757  0.282506  0.520515  0.453185  0.015696 -0.402799   \n",
      "7  0.642823 -0.454932  0.495139  0.344915  0.325437 -0.133436 -0.354844   \n",
      "8  0.051358 -0.654712  0.681238  0.581920  0.089366 -0.078336 -0.125406   \n",
      "9  0.304208 -0.483646  0.337412  0.288739  0.278590 -0.229991 -0.230316   \n",
      "\n",
      "         7         8         9   ...        40        41        42        43  \\\n",
      "0  0.378647  0.039353 -0.193295  ... -0.103244  0.205858 -0.718903 -0.672153   \n",
      "1  0.182156 -0.242867 -0.412083  ...  0.210736 -0.065111 -0.790595 -0.731772   \n",
      "2  0.230072  0.387936 -0.131948  ...  0.320209 -0.066394 -0.355472 -0.500170   \n",
      "3  0.168010  0.457667 -0.085995  ...  0.409446  0.056393 -0.455931 -0.433630   \n",
      "4 -0.225012  0.264453 -0.282406  ...  0.344880  0.341708 -0.339324 -0.930270   \n",
      "5  0.221435  0.397197 -0.029377  ...  0.306947  0.035885 -0.341933 -0.574027   \n",
      "6 -0.465193  0.046339 -0.627397  ...  0.525563  0.483601 -0.523068 -1.417330   \n",
      "7 -0.205972  0.307851 -0.367820  ...  0.447928  0.057165 -0.469231 -0.530632   \n",
      "8 -0.279161 -0.014397 -0.335367  ...  0.162736  0.300331 -0.306205 -1.241914   \n",
      "9  0.079696  0.432337 -0.059810  ...  0.303447  0.004462 -0.384716 -0.469938   \n",
      "\n",
      "         44        45        46        47        48        49  \n",
      "0  0.594621 -0.264274  0.348219  0.032280  0.225506  0.636500  \n",
      "1  0.831006 -0.055449  0.497038 -0.465614  0.442611  0.295222  \n",
      "2  0.829056  0.062247  0.391740 -0.150111  0.255369  0.048886  \n",
      "3  0.731613 -0.180634  0.450513 -0.406681  0.284721  0.117411  \n",
      "4  1.003975 -0.382748 -0.011729 -0.163109  0.631745 -0.052006  \n",
      "5  0.746498  0.031722  0.154326 -0.159828  0.294364 -0.033282  \n",
      "6  1.019539 -0.491547  1.043858 -0.236448  0.729291 -0.786848  \n",
      "7  0.947882 -0.247634  0.629945 -0.176400  0.488021 -0.078945  \n",
      "8  1.131881 -0.068271 -0.056366 -0.073879  0.672074 -0.088103  \n",
      "9  0.668387  0.047129  0.263146 -0.132135  0.245433 -0.004770  \n",
      "\n",
      "[10 rows x 50 columns]\n",
      "(907, 50)\n"
     ]
    }
   ],
   "source": [
    "list_of_sent = X_test[\"Text\"]\n",
    "\n",
    "listof_sent_vec=[]\n",
    "#tqdm is for improving speed\n",
    "#Vectorization and normalization both going on\n",
    "for sent in tqdm(list_of_sent): \n",
    "    sent_vec = np.zeros(50) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    listof_sent_vec.append(sent_vec)\n",
    "    \n",
    "list_col=tuple(range(50))\n",
    "X_test=pd.DataFrame(data=listof_sent_vec, columns=list_col)\n",
    "print(X_test.head(10))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01897236  1.35352734  0.02189467  0.03848828 -0.25931455  0.65818892\n",
      "  0.27816396 -0.88346469 -0.19936649 -0.47473253 -0.17035279  0.98752834\n",
      " -0.53518416  0.77356854  0.01376524 -0.13505271  0.25711098  0.55351811\n",
      "  0.07613625 -0.77118451  0.46461331  0.43185612 -0.05275839  0.00925503\n",
      " -0.61257239  0.39285602 -0.54316702  0.40757853 -0.72047777 -0.23868149\n",
      " -0.06038479 -0.45479151 -0.32938002 -0.39707486  0.44504935  0.57017544\n",
      "  0.09032992 -0.66409583  0.73949395 -0.26850521  0.13615461  0.3389129\n",
      "  0.1257821  -0.10969215  0.22905093 -0.69413646  0.59064034  0.61720237\n",
      "  0.23423148 -0.01190825]\n",
      "[-1.99898498 -1.90712679 -1.39671151  1.30870904  0.2236665  -1.55924123\n",
      "  2.3988228   0.6414891  -1.09292678  0.13279559  1.52709884 -1.32633582\n",
      "  2.7139989   0.41771387 -0.57255367  1.21710168  0.35531865 -1.46169719\n",
      "  2.66288363  0.27392521  1.13362176 -1.2191306  -0.64300055  0.84897975\n",
      " -0.6699157  -0.25853123  1.62421362 -1.16586129  0.6693948   2.63686826\n",
      "  1.2818412  -0.56842676  0.46314445  1.19827138  0.78322603 -1.5896679\n",
      " -1.80514362  0.60068899 -1.07023538  1.80842134 -3.25260885  1.07805558\n",
      " -1.42739179 -0.60101241 -0.7441792  -1.11772069 -0.71135882  1.22535755\n",
      " -0.24008025  3.05242057]\n"
     ]
    }
   ],
   "source": [
    "#Standardization\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train[0])\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of alpha after 10-fold CV: 0.1\n",
      "Accuracy of Naive Bayes: 65.38037486218302\n",
      "Confusion matrix for Naive Bayes: \n",
      " [[307 152]\n",
      " [162 286]]\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes(using 10-fold cv for alpha)\n",
    "\n",
    "from sklearn import metrics;\n",
    "from sklearn import naive_bayes;\n",
    "\n",
    "#k-fold cv\n",
    "alpha_values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,1]\n",
    "cv_scores=[]\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    nb_model=naive_bayes.BernoulliNB(alpha=alpha)\n",
    "    scores=model_selection.cross_val_score(nb_model,X_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "max_acc=cv_scores[0]\n",
    "i=0\n",
    "max_i=0\n",
    "for acc in cv_scores:\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        max_i=i\n",
    "    i=i+1\n",
    "    \n",
    "optimal_alpha=alpha_values[max_i]\n",
    "print(\"Optimal value of alpha after 10-fold CV: \"+str(optimal_alpha))\n",
    "\n",
    "#Final model\n",
    "nb_model=naive_bayes.BernoulliNB(alpha=optimal_alpha)\n",
    "nb_model.fit(X_train,y_train)\n",
    "\n",
    "arr=nb_model.predict(X_test)\n",
    "\n",
    "acc_nb=metrics.accuracy_score(y_test, arr, normalize=True) * float(100)\n",
    "cf_mat_NB=metrics.confusion_matrix(y_test,arr)\n",
    "print(\"Accuracy of Naive Bayes: \"+str(acc_nb))\n",
    "print(\"Confusion matrix for Naive Bayes: \\n\",cf_mat_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for LR:  79.27232635060639\n",
      "The confusion_matrix for LR: \n",
      " [[392  67]\n",
      " [121 327]]\n"
     ]
    }
   ],
   "source": [
    "#Logostic Regresion(using grid search for parameter)\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#model and grid search\n",
    "tuned_parameter=[ { 'C' : [10**-4,10**-2,10**0,10**2,10**4] } ]\n",
    "LR_model=model_selection.GridSearchCV(LogisticRegression(solver=\"liblinear\"),tuned_parameter,scoring='f1',cv=5)\n",
    "LR_model.fit(X_train,y_train)\n",
    "\n",
    "arr=LR_model.predict(X_test)\n",
    "cf_mat_LR=metrics.confusion_matrix(y_test,arr)\n",
    "acc_LR=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for LR: \", acc_LR)\n",
    "print(\"The confusion_matrix for LR: \\n\", cf_mat_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of d after 10-fold CV: 9\n",
      "Accuracy for DT: 84.6747519294377\n",
      "Confusion matrix for DT: \n",
      " [[426  33]\n",
      " [106 342]]\n"
     ]
    }
   ],
   "source": [
    "#Decision Tress\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "#Finding right depth\n",
    "d_values=range(1,10)\n",
    "cv_scores=[]\n",
    "\n",
    "for d in d_values:\n",
    "    DT=tree.DecisionTreeClassifier(max_depth=d)\n",
    "    scores=model_selection.cross_val_score(DT,X_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "max_acc=cv_scores[0]\n",
    "i=0\n",
    "max_i=0\n",
    "for acc in cv_scores:\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        max_i=i\n",
    "    i=i+1\n",
    "    \n",
    "optimal_d=d_values[max_i]\n",
    "print(\"Optimal value of d after 10-fold CV: \"+str(optimal_d))\n",
    "\n",
    "#Model\n",
    "DT_model=tree.DecisionTreeClassifier(max_depth=optimal_d)\n",
    "DT_model.fit(X_train,y_train)\n",
    "\n",
    "arr=DT_model.predict(X_test)\n",
    "\n",
    "final_acc_DT=metrics.accuracy_score(y_test, arr, normalize=True) * float(100)\n",
    "cf_mat_DT=metrics.confusion_matrix(y_test,arr)\n",
    "print(\"Accuracy for DT: \"+str(final_acc_DT))\n",
    "print(\"Confusion matrix for DT: \\n\",cf_mat_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of k after 10-fold CV: 1\n",
      "Accuracy for KNN: 95.25909592061743\n",
      "Confusion matrix for KNN: \n",
      " [[459   0]\n",
      " [ 43 405]]\n"
     ]
    }
   ],
   "source": [
    "#KNN(using 10-fold cv for k)\n",
    "\n",
    "from sklearn import neighbors;\n",
    "from sklearn import model_selection;\n",
    "from sklearn import metrics;\n",
    "\n",
    "#k-fold cv\n",
    "k_values=range(1,50,2)\n",
    "cv_scores=[]\n",
    "\n",
    "for k in k_values:\n",
    "    knn=neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    scores=model_selection.cross_val_score(knn,X_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "max_acc=cv_scores[0]\n",
    "i=0\n",
    "max_i=0\n",
    "for acc in cv_scores:\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        max_i=i\n",
    "    i=i+1\n",
    "    \n",
    "optimal_k=k_values[max_i]\n",
    "print(\"Optimal value of k after 10-fold CV: \"+str(optimal_k))\n",
    "\n",
    "#Final_model\n",
    "knn_model=neighbors.KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn_model.fit(X_train,y_train)\n",
    "\n",
    "arr=knn_model.predict(X_test)\n",
    "\n",
    "final_acc_knn=metrics.accuracy_score(y_test, arr, normalize=True) * float(100)\n",
    "cf_mat_knn=metrics.confusion_matrix(y_test,arr)\n",
    "print(\"Accuracy for KNN: \"+str(final_acc_knn))\n",
    "print(\"Confusion matrix for KNN: \\n\",cf_mat_knn)\n",
    "\n",
    "#KNN proved to be a better model than NB,LR,DT and notedly its ability to properly classify the positive points is great.\n",
    "#It is a good candidate to be used as a first model for stacking/cascading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for SVM:  94.8180815876516\n",
      "The confusion_matrix for SVM: \n",
      " [[459   0]\n",
      " [ 47 401]]\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "SVM_model=SVC(C=1000,kernel=\"rbf\",gamma=\"auto\")\n",
    "SVM_model.fit(X_train,y_train)\n",
    "\n",
    "arr=SVM_model.predict(X_test)\n",
    "cf_mat_SVM=metrics.confusion_matrix(y_test,arr)\n",
    "acc_SVM=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for SVM: \", acc_SVM)\n",
    "print(\"The confusion_matrix for SVM: \\n\", cf_mat_SVM)\n",
    "\n",
    "#Observed that Linear SVM performs similarly as LR and not much effective\n",
    "##RBF Kernel offered an accuracy of 92% at C=1000(behaves like knn)\n",
    "#Poly Kernel had 77% accuracy and looked not so good for negative pts at degree=4 and C=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for GBDT:  89.41565600882029\n",
      "The confusion_matrix for GBDT: \n",
      " [[432  27]\n",
      " [ 69 379]]\n"
     ]
    }
   ],
   "source": [
    "#GBDT\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "from sklearn import metrics\n",
    "\n",
    "GBDT_model=GBDT(loss=\"deviance\",learning_rate=0.3,n_estimators=100)\n",
    "GBDT_model.fit(X_train,y_train)\n",
    "\n",
    "arr=GBDT_model.predict(X_test)\n",
    "cf_mat_GBDT=metrics.confusion_matrix(y_test,arr)\n",
    "acc_GBDT=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for GBDT: \", acc_GBDT)\n",
    "print(\"The confusion_matrix for GBDT: \\n\", cf_mat_GBDT)\n",
    "\n",
    "#GBDT performed reasonably well on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for RF:  98.34619625137817\n",
      "The confusion_matrix for RF: \n",
      " [[459   0]\n",
      " [ 15 433]]\n"
     ]
    }
   ],
   "source": [
    "#Random Forests(implementing bagging)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "RF_model=RandomForestClassifier(n_estimators=1000,max_depth=None)\n",
    "RF_model.fit(X_train,y_train)\n",
    "\n",
    "arr=RF_model.predict(X_test)\n",
    "cf_mat_RF=metrics.confusion_matrix(y_test,arr)\n",
    "acc_RF=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for RF: \", acc_RF)\n",
    "print(\"The confusion_matrix for RF: \\n\", cf_mat_RF)\n",
    "\n",
    "#RF proved to be the best classifier till now with an accuracy of 98% at 1000 trees.\n",
    "#It classifies all positive points correctly "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
