{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Score, dtype: int64\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  1.271121 -1.282555 -1.315072  1.416699 -1.343868 -1.282359  1.450958   \n",
      "1 -1.255075  1.183532  1.229887 -1.273021  1.130575  1.126262 -1.385114   \n",
      "2 -2.346429  2.431473  2.336246 -2.263446  2.357211  2.158313 -2.221263   \n",
      "3 -1.453606  1.413520  1.453984 -1.528507  1.462599  1.214627 -1.495196   \n",
      "4 -1.277107  1.144231  1.258039 -1.299785  1.212882  1.342687 -1.179163   \n",
      "\n",
      "          7         8         9  ...        40        41        42        43  \\\n",
      "0 -1.151886  1.409491 -1.176996  ... -1.275696  0.947599 -1.272468 -1.309511   \n",
      "1  0.657298 -1.169387  1.193175  ...  1.253955 -1.448713  1.183008  1.241729   \n",
      "2  2.087082 -2.384908  2.219964  ...  2.304383 -2.012602  2.263938  2.266795   \n",
      "3  1.533178 -1.484417  1.489940  ...  1.463652 -1.343887  1.459697  1.416326   \n",
      "4  1.065217 -1.283690  1.263329  ...  1.221818 -0.961523  1.158105  1.291899   \n",
      "\n",
      "         44        45        46        47        48        49  \n",
      "0  1.309864 -1.276141  1.350802  1.323176 -1.371580  1.304752  \n",
      "1 -1.195933  1.241520 -1.198372 -1.167748  1.197943 -1.184675  \n",
      "2 -2.296178  2.356821 -2.240935 -2.223307  2.089361 -2.305504  \n",
      "3 -1.423263  1.447549 -1.448111 -1.451110  1.455097 -1.488243  \n",
      "4 -1.254562  1.215403 -1.222332 -1.172742  1.181958 -1.214406  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "#Basic importing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn import model_selection\n",
    "\n",
    "data=pd.read_csv(\"Amazon_reviews_vectorized.csv\")\n",
    "data=data.drop([\"Unnamed: 0\",\"Time\"],axis=1)\n",
    "\n",
    "#upsampling\n",
    "data1=data[data[\"Score\"]==1]\n",
    "data0=data[data[\"Score\"]==0]\n",
    "data0new=pd.concat([data0,data0,data0,data0,data0],ignore_index=True)\n",
    "\n",
    "data=pd.concat([data1,data0new],ignore_index=True)\n",
    "\n",
    "#Standardization\n",
    "\n",
    "data_L=data[\"Score\"]\n",
    "data=data.drop(\"Score\",axis=1)\n",
    "\n",
    "cols=data.columns\n",
    "datastd=StandardScaler().fit_transform(data)\n",
    "data=pd.DataFrame(data=datastd, columns=cols)\n",
    "\n",
    "print(data_L.head())\n",
    "print(data.head())\n",
    "\n",
    "#Breaking data\n",
    "X_train,X_test,y_train,y_test=model_selection.train_test_split(data,data_L,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of alpha after 10-fold CV: 0.1\n",
      "Accuracy of Naive Bayes: 54.601226993865026\n",
      "Confusion matrix for Naive Bayes: \n",
      " [[45 32]\n",
      " [42 44]]\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes(using 10-fold cv for alpha)\n",
    "\n",
    "from sklearn import model_selection;\n",
    "from sklearn import metrics;\n",
    "from sklearn import naive_bayes;\n",
    "\n",
    "#k-fold cv\n",
    "alpha_values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,1]\n",
    "cv_scores=[]\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    nb_model=naive_bayes.BernoulliNB(alpha=alpha)\n",
    "    scores=model_selection.cross_val_score(nb_model,X_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "max_acc=cv_scores[0]\n",
    "i=0\n",
    "max_i=0\n",
    "for acc in cv_scores:\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        max_i=i\n",
    "    i=i+1\n",
    "    \n",
    "optimal_alpha=alpha_values[max_i]\n",
    "print(\"Optimal value of alpha after 10-fold CV: \"+str(optimal_alpha))\n",
    "\n",
    "#Final model\n",
    "nb_model=naive_bayes.BernoulliNB(alpha=optimal_alpha)\n",
    "nb_model.fit(X_train,y_train)\n",
    "\n",
    "arr=nb_model.predict(X_test)\n",
    "\n",
    "acc_nb=metrics.accuracy_score(y_test, arr, normalize=True) * float(100)\n",
    "cf_mat_NB=metrics.confusion_matrix(y_test,arr)\n",
    "print(\"Accuracy of Naive Bayes: \"+str(acc_nb))\n",
    "print(\"Confusion matrix for Naive Bayes: \\n\",cf_mat_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for LR:  63.190184049079754\n",
      "The confusion_matrix for LR: \n",
      " [[45 32]\n",
      " [28 58]]\n"
     ]
    }
   ],
   "source": [
    "#Logostic Regresion(using grid search for parameter)\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#model and grid search\n",
    "tuned_parameter=[ { 'C' : [10**-4,10**-2,10**0,10**2,10**4] } ]\n",
    "LR_model=model_selection.GridSearchCV(LogisticRegression(solver=\"liblinear\"),tuned_parameter,scoring='f1',cv=5)\n",
    "LR_model.fit(X_train,y_train)\n",
    "\n",
    "arr=LR_model.predict(X_test)\n",
    "cf_mat_LR=metrics.confusion_matrix(y_test,arr)\n",
    "acc_LR=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for LR: \", acc_LR)\n",
    "print(\"The confusion_matrix for LR: \\n\", cf_mat_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of d after 10-fold CV: 9\n",
      "Accuracy for DT: 82.82208588957054\n",
      "Confusion matrix for DT: \n",
      " [[77  0]\n",
      " [28 58]]\n"
     ]
    }
   ],
   "source": [
    "#Decision Tress\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "#Finding right depth\n",
    "d_values=range(1,10)\n",
    "cv_scores=[]\n",
    "\n",
    "for d in d_values:\n",
    "    DT=tree.DecisionTreeClassifier(max_depth=d)\n",
    "    scores=model_selection.cross_val_score(DT,X_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "max_acc=cv_scores[0]\n",
    "i=0\n",
    "max_i=0\n",
    "for acc in cv_scores:\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        max_i=i\n",
    "    i=i+1\n",
    "    \n",
    "optimal_d=d_values[max_i]\n",
    "print(\"Optimal value of d after 10-fold CV: \"+str(optimal_d))\n",
    "\n",
    "#Model\n",
    "DT_model=tree.DecisionTreeClassifier(max_depth=optimal_d)\n",
    "DT_model.fit(X_train,y_train)\n",
    "\n",
    "arr=DT_model.predict(X_test)\n",
    "\n",
    "final_acc_DT=metrics.accuracy_score(y_test, arr, normalize=True) * float(100)\n",
    "cf_mat_DT=metrics.confusion_matrix(y_test,arr)\n",
    "print(\"Accuracy for DT: \"+str(final_acc_DT))\n",
    "print(\"Confusion matrix for DT: \\n\",cf_mat_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of k after 10-fold CV: 1\n",
      "Accuracy for KNN: 92.63803680981594\n",
      "Confusion matrix for KNN: \n",
      " [[77  0]\n",
      " [12 74]]\n"
     ]
    }
   ],
   "source": [
    "#KNN(using 10-fold cv for k)\n",
    "\n",
    "from sklearn import neighbors;\n",
    "from sklearn import model_selection;\n",
    "from sklearn import metrics;\n",
    "\n",
    "#k-fold cv\n",
    "k_values=range(1,50,2)\n",
    "cv_scores=[]\n",
    "\n",
    "for k in k_values:\n",
    "    knn=neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    scores=model_selection.cross_val_score(knn,X_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "max_acc=cv_scores[0]\n",
    "i=0\n",
    "max_i=0\n",
    "for acc in cv_scores:\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        max_i=i\n",
    "    i=i+1\n",
    "    \n",
    "optimal_k=k_values[max_i]\n",
    "print(\"Optimal value of k after 10-fold CV: \"+str(optimal_k))\n",
    "\n",
    "#Final_model\n",
    "knn_model=neighbors.KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn_model.fit(X_train,y_train)\n",
    "\n",
    "arr=knn_model.predict(X_test)\n",
    "\n",
    "final_acc_knn=metrics.accuracy_score(y_test, arr, normalize=True) * float(100)\n",
    "cf_mat_knn=metrics.confusion_matrix(y_test,arr)\n",
    "print(\"Accuracy for KNN: \"+str(final_acc_knn))\n",
    "print(\"Confusion matrix for KNN: \\n\",cf_mat_knn)\n",
    "\n",
    "#KNN proved to be a better model than NB,LR,DT and notedly its ability to properly classify the negative points is great.\n",
    "#It is a good candidate to be used as a first model for stacking/cascading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for SVM:  59.50920245398773\n",
      "The confusion_matrix for SVM: \n",
      " [[19 58]\n",
      " [ 8 78]]\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "SVM_model=SVC(C=100,kernel=\"poly\",degree=4,gamma=\"auto\")\n",
    "SVM_model.fit(X_train,y_train)\n",
    "\n",
    "arr=SVM_model.predict(X_test)\n",
    "cf_mat_SVM=metrics.confusion_matrix(y_test,arr)\n",
    "acc_SVM=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for SVM: \", acc_SVM)\n",
    "print(\"The confusion_matrix for SVM: \\n\", cf_mat_SVM)\n",
    "\n",
    "#Observed that Linear SVM performs similarly as LR and not much effective\n",
    "##RBF Kernel offered an accuracy of 100% for the negative class points but only 75% for positive pts at C=2000(behaves like knn)\n",
    "#Poly Kernel offered an ok ok accuracy for negative but dumb for positive at degree=2 and C=1000\n",
    "#Poly Kernel did okay for positive but behaved terrible for negative pts at degree=4 and C=100\n",
    "#No SVM model did an overall good job on both classes so we will use them in cascading/stacking in some way or other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for GBDT:  92.63803680981594\n",
      "The confusion_matrix for GBDT: \n",
      " [[77  0]\n",
      " [12 74]]\n"
     ]
    }
   ],
   "source": [
    "#GBDT\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "from sklearn import metrics\n",
    "\n",
    "GBDT_model=GBDT(loss=\"deviance\",learning_rate=0.3,n_estimators=100)\n",
    "GBDT_model.fit(X_train,y_train)\n",
    "\n",
    "arr=GBDT_model.predict(X_test)\n",
    "cf_mat_GBDT=metrics.confusion_matrix(y_test,arr)\n",
    "acc_GBDT=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for GBDT: \", acc_GBDT)\n",
    "print(\"The confusion_matrix for GBDT: \\n\", cf_mat_GBDT)\n",
    "\n",
    "#GBDT performed reasonably well on the data\n",
    "#It performs with full accuracy on negative pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for RF:  98.77300613496932\n",
      "The confusion_matrix for RF: \n",
      " [[77  0]\n",
      " [ 2 84]]\n"
     ]
    }
   ],
   "source": [
    "#Random Forests(implementing bagging)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "RF_model=RandomForestClassifier(n_estimators=1000,max_depth=None)\n",
    "RF_model.fit(X_train,y_train)\n",
    "\n",
    "arr=RF_model.predict(X_test)\n",
    "cf_mat_RF=metrics.confusion_matrix(y_test,arr)\n",
    "acc_RF=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for RF: \", acc_RF)\n",
    "print(\"The confusion_matrix for RF: \\n\", cf_mat_RF)\n",
    "\n",
    "#RF proved to be the best classifier till now with an accuracy of 95-98% at 1000 trees.\n",
    "#It classifies all negative points correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE WILL BE USING TECHNIQUES THAT WILL COMBINE THE ABOVE MODEL TO FORM A BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for SVM+GBDT+RF:  98.77300613496932\n",
      "The confusion_matrix for SVM+GBDT+RF: \n",
      " [[77  0]\n",
      " [ 2 84]]\n"
     ]
    }
   ],
   "source": [
    "#Stacking models\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "LR=LR_model\n",
    "\n",
    "SVM_=SVC(C=100,kernel=\"poly\",degree=4,gamma=\"auto\")\n",
    "GBDT_=GBDT(loss=\"deviance\",learning_rate=0.3,n_estimators=100)\n",
    "RF_=RandomForestClassifier(n_estimators=1000,max_depth=None)\n",
    "\n",
    "Stacked_model=StackingClassifier(classifiers=[SVM_,GBDT_,RF_], meta_classifier=LR)\n",
    "Stacked_model.fit(X_train,y_train)\n",
    "\n",
    "arr=Stacked_model.predict(X_test)\n",
    "\n",
    "cf_mat_NBLR=metrics.confusion_matrix(y_test,arr)\n",
    "acc_NBLR=metrics.accuracy_score(y_test,arr)*float(100)\n",
    "print(\"The accuracy for SVM+GBDT+RF: \", acc_NBLR)\n",
    "print(\"The confusion_matrix for SVM+GBDT+RF: \\n\", cf_mat_NBLR)\n",
    "\n",
    "#We got the best accuracy by stacking SVM, GBDT and RF with LR as meta classifier\n",
    "#The accuracy was similar to random forest except a few cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
